{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñ LLM Basics - Interactive Tutorial\n",
    "\n",
    "This notebook provides hands-on exploration of Large Language Models.\n",
    "\n",
    "## What You'll Learn\n",
    "1. Understanding LLM fundamentals\n",
    "2. Working with tokenization\n",
    "3. Prompt engineering basics\n",
    "4. Model comparison\n",
    "\n",
    "**Prerequisites**: OpenAI API key (optional for some examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install openai transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Tokenization\n",
    "\n",
    "Tokenization is how text is broken into pieces that models can understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Example text\n",
    "text = \"Large Language Models are transforming AI!\"\n",
    "\n",
    "# Tokenize\n",
    "tokens = tokenizer.tokenize(text)\n",
    "token_ids = tokenizer.encode(text)\n",
    "\n",
    "print(f\"Original text: {text}\")\n",
    "print(f\"\\nTokens: {tokens}\")\n",
    "print(f\"Number of tokens: {len(tokens)}\")\n",
    "print(f\"\\nToken IDs: {token_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try it yourself!\n",
    "Modify the text below and see how it tokenizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "your_text = \"Write your text here...\"\n",
    "your_tokens = tokenizer.tokenize(your_text)\n",
    "print(f\"Your text: {your_text}\")\n",
    "print(f\"Tokens: {your_tokens}\")\n",
    "print(f\"Token count: {len(your_tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prompt Engineering Basics\n",
    "\n",
    "The way you phrase prompts significantly affects LLM outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example prompts - from vague to specific\n",
    "prompts = [\n",
    "    \"Explain AI\",\n",
    "    \"Explain AI in simple terms\",\n",
    "    \"Explain AI to a 10-year-old using an analogy\",\n",
    "    \"You are a teacher. Explain AI to a 10-year-old student using a cooking analogy. Keep it under 3 sentences.\"\n",
    "]\n",
    "\n",
    "print(\"Prompt Engineering Examples:\\n\")\n",
    "for i, prompt in enumerate(prompts, 1):\n",
    "    print(f\"{i}. {prompt}\")\n",
    "    print(f\"   Specificity: {'‚≠ê' * i}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Prompt Engineering Principles\n",
    "\n",
    "1. **Be Specific**: Clear instructions yield better results\n",
    "2. **Provide Context**: Set the role, tone, and format\n",
    "3. **Use Examples**: Few-shot prompting improves accuracy\n",
    "4. **Iterate**: Refine prompts based on outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Parameters\n",
    "\n",
    "Understanding key parameters that control LLM behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Common parameters\n",
    "params = {\n",
    "    'Parameter': ['temperature', 'max_tokens', 'top_p', 'frequency_penalty'],\n",
    "    'Range': ['0-2', '1-4096', '0-1', '-2 to 2'],\n",
    "    'Effect': [\n",
    "        'Randomness (0=deterministic, 2=very random)',\n",
    "        'Maximum length of response',\n",
    "        'Nucleus sampling (diversity of tokens)',\n",
    "        'Reduces repetition (higher=less repetition)'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(params)\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. With OpenAI API (Optional)\n",
    "\n",
    "If you have an OpenAI API key, you can test live completions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your API key\n",
    "# os.environ['OPENAI_API_KEY'] = 'your-key-here'\n",
    "\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "    \n",
    "    if os.getenv('OPENAI_API_KEY'):\n",
    "        client = OpenAI()\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": \"Explain transformers in one sentence.\"}\n",
    "            ],\n",
    "            max_tokens=50\n",
    "        )\n",
    "        \n",
    "        print(\"Response:\", response.choices[0].message.content)\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è OpenAI API key not set - skipping this example\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ÑπÔ∏è OpenAI example skipped: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparing Context Windows\n",
    "\n",
    "Different models have different context window sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "models = ['GPT-3.5', 'GPT-4', 'GPT-4-Turbo', 'Claude-2', 'Claude-3']\n",
    "context_sizes = [4096, 8192, 128000, 100000, 200000]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(models, context_sizes, color='skyblue')\n",
    "plt.xlabel('Context Window Size (tokens)')\n",
    "plt.title('LLM Context Window Comparison')\n",
    "plt.xscale('log')\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "\n",
    "for i, v in enumerate(context_sizes):\n",
    "    plt.text(v, i, f' {v:,}', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Key Takeaways\n",
    "\n",
    "‚úÖ **Tokenization** breaks text into processable pieces  \n",
    "‚úÖ **Prompts** should be clear, specific, and well-structured  \n",
    "‚úÖ **Parameters** control output randomness and length  \n",
    "‚úÖ **Context windows** limit how much text models can process  \n",
    "‚úÖ **Practice** improves prompt engineering skills  \n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. Experiment with different prompts\n",
    "2. Try temperature variations\n",
    "3. Explore RAG for knowledge-grounded responses\n",
    "4. Build AI agents with tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîó Resources\n",
    "\n",
    "- [OpenAI Prompt Engineering Guide](https://platform.openai.com/docs/guides/prompt-engineering)\n",
    "- [Hugging Face Transformers](https://huggingface.co/docs/transformers)\n",
    "- [Anthropic Prompt Library](https://docs.anthropic.com/claude/prompt-library)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
